\chapter{Probability concepts. Black-Scholes formula. Greeks and Hedging.}

\section{Discrete probability concepts}
Let $ S = \{ s_1, s_2, ..., s_n \} $ be a finite set and let
    $ P : S \rightarrow [0, 1] $ be a probability function defined on $ S $,
    i.e., a function with the following properites:
\begin{equation}
    P(s_i) \geq 0,\ \forall i = 1 : n, \quad \text{and}
        \sum_{i=1}^{n} P(s_i) = 1.
    \label{eq:probability-distribution}
\end{equation}
Any function $ X : S \rightarrow \mathbb{R} $ is called a random variable
    defined on the set of outcomes $ S $.

\begin{definition}
    Let $ X : S \rightarrow \mathbb{R} $ be a random variable on the set $ S $
        endowed with a probability function $ P : S \rightarrow [0, 1] $.
    The expected value $ E[X] $ of $ X $ (also called the mean of $ X $) is
    \begin{equation}
        E[X] = \sum_{i=1}^{n} P(s_i) X(s_i).
        \label{eq:expected-value}
    \end{equation}
    The variance $ var(X) $ of $ X $ is
    \begin{equation}
        var(X) = E[(X - E[X])^2].
        \label{eq:variance}
    \end{equation}
    The standard deviation of $ \sigma(X) $ of $ X $ is
    \begin{equation}
        \sigma(x) = \sqrt{var(X)}.
        \label{eq:standard-deviation}
    \end{equation}
\end{definition}

\begin{lemma}
    Let $ X : S \rightarrow \mathbb{R} $ be a random variable on the set $ S $
        endowed with a probability function $ P : S \rightarrow [0, 1] $.
    Then,
    \begin{equation*}
        var(X) = E[X^2] - (E[X])^2 = \sum_{i=1}^{n} P(s_i) (X(s_i))^2 -
            (E[X])^2.
    \end{equation*}
\end{lemma}

\section{Continuous probability concepts}
Let $ S $ be a set, and consider a family $ \mathbb{F} $ of subsets of $ S $
    such that
\begin{enumerate}
    \item $ S \in \mathbb{F}; $
    \item if $ A \in \mathbb{F} $, then $ S \setminus A \in \mathbb{F} $;
    \item if $ A_k \in \mathbb{F} $, $ k=1:\infty $, then
        $ \bigcup_{k=1}^{\infty} A_k \in \mathbb{F} $.
\end{enumerate}

The set $ S $ represents the sample space, any element of $ S $ is a possible
    outcome, and a set $ A \in \mathbb{F} $ is an event, i.e., a collection of
    possible outcomes.

A probability function $ P : \mathbb{F} \rightarrow [0, 1] $ defined
    $ \mathbb{F} $ is a nonnegative function such that $ P(S) = 1 $ and
\begin{equation*}
    P \left( \bigcup_{k=1}^{\infty} A_k \right) = \sum_{k=1}^{\infty} P(A_k),
        \quad \forall A_k \in \mathbb{F}, k = 1 : \infty,
        \quad \forall A_i \cap A_j = \emptyset, i \neq j.
\end{equation*}

A function $ X : S \rightarrow \mathbb{R} $ is called a random variable if
\begin{equation*}
    \{ s \in S\ \text{such that}\ X(s) \leq t \} \in \mathbb{F}, \quad
        \forall t \in \mathbb{R}.
\end{equation*}

The cumulative distribution function $ F : \mathbb{R} \rightarrow [0, 1] $ of
    the random variable $ X $ is defined as
\begin{equation}
    F(t) = P(X \leq t).
\end{equation}

For any random variable $ X $ considered here, we assume that $ X $ has a
    probability density function $ f(x) $, i.e., we assume that there exists a
    function $ f : \mathbb{R} \rightarrow \mathbb{R} $ such that
\begin{equation}
    P(a \leq X \leq b) = \int_{a}^{b} f(x) dx.
\end{equation}

For an integrable function $ f(x) $ to be the probability density function of a
    random variable, it is necessary and sufficient that $ f(x) \geq 0 $, for
    all $ x \in \mathbb{R} $, and
\begin{equation}
    \int_{-\infty}^{\infty} f(x) dx = 1.
\end{equation}

\begin{lemma}
    Let $ f(x) $ be the probability density function of the random variable
        $ X $.
    Then the cumulative distribution function of $ X $ can be written as
    \begin{equation}
        F(t) = \int_{-\infty}^{t} f(x) dx.
        \label{eq:cumulative-distribution-function}
    \end{equation}
\end{lemma}

\begin{definition}
    Let $ f(x) $ be the probability density function of the random variable
        $ X $.
    The expected value $ E[X] $ of $ X $ (also called the mean of $ X $) is
        defined as
    \begin{equation}
        E[X] = \int_{-\infty}^{\infty} x f(x) dx.
        \label{eq:continuous-expected-value}
    \end{equation}
\end{definition}
The expected value of a random variable is a linear operator:
\begin{lemma}
    Let $ X $ and $ Y $ be two random variables over the same probability
        space.
    Then
    \begin{align}
        E[X + Y] &= E[X] + E[Y]; \\
        E[cX] &= c E[X], \quad \forall c \in \mathbb{R}.
    \end{align}
\end{lemma}

\begin{lemma}
    Let $ h : \mathbb{R} \rightarrow \mathbb{R} $ be a piecewise continuous
        function, and let $ X : S \rightarrow \mathbb{R} $ be a random
        variable with probability density function $ f(x) $ such that
        $ \int_{\mathbb{R}} | h(x) | f(x) dx < \infty $.
    Then $ h(X) : S \rightarrow \mathbb{R} $ is a random variable and the
        expected value of $ h(X) $ is
    \begin{equation}
        E[h(X)] = \int_{-\infty}^{\infty} h(x) f(x) dx.
        \label{eq:piecewise-continuous-expected-value}
    \end{equation}
\end{lemma}

\subsection{Variance, covariance, and correlation}
\begin{definition}
    Let $ f(x) $ be the probability density function of the random variable
        $ X $, and assume that $ \int_{\mathbb{R}} x^2 f(x) dx < 0 $.
    Let $ m = E[X] $ denote the expected value of $ X $.
    The variance $ var(X) $ of $ X $ is defined as
    \begin{equation}
        var(X) = E[(X - E[X])^2] = \int_{-\infty}^{\infty} (x - m)^2 f(x) dx.
        \label{eq:continuous-variance}
    \end{equation}
    The standard deviation $ \sigma(X) $ of $ X $ is defined as
    \begin{equation}
        \sigma(X) = \sqrt{var(X)}.
        \label{eq:continuous-standard-deviation}
    \end{equation}
    Therefore, $ var(X) = (\sigma(X))^2 = \sigma^2(X) $.
\end{definition}

From \eqref{eq:continuous-variance} and
    \eqref{eq:continuous-standard-deviation}, it is easy to see that
\begin{align}
    var(c X) &= c^2 var(X), \quad \forall c \in \mathbb{R}; \\
    \sigma(c X) &= |c| \sigma(X), \quad \forall c \in \mathbb{R}.
\end{align}

\begin{lemma}
    If $ X $ is a random variable, then
    \begin{equation*}
        var(X) = E[X^2] - (E[X])^2.
    \end{equation*}
\end{lemma}

\begin{definition}
    Let $ X $ and $ Y $ be two random variables over the sample probability
        space.
    The covariance $ cov(X, Y) $ of $ X $ and $ Y $ is defined as
    \begin{equation}
        cov(X, Y) = E[(X - E[X]) (Y - E[Y])].
        \label{eq:continuous-covariance}
    \end{equation}
    The correlation $ corr(X, Y) $ between $ X $ and $ Y $ is equal to the
        covariance of $ X $ and $ Y $ normalized with respect to the standard
        deviations of $ X $ and $ Y $, i.e.,
    \begin{equation}
        corr(X, Y) = \frac{cov(X, Y)}{\sigma(X) \sigma(Y)},
        \label{eq:continuous-correlation}
    \end{equation}
    where $ \sigma(X) $ and $ \sigma(Y) $ are the standard deviations of $ X $
        and $ Y $, respectively.
\end{definition}

\begin{lemma}
    Let $ X $ and $ Y $ be two random variables over the same probability space.
    Then
    \begin{equation}
        cov(X, Y) = E[XY] - E[X]E[Y].
    \end{equation}
\end{lemma}

\begin{lemma}
    Let $ X $ and $ Y $ be two random variables over the same probability space.
    Then
    \begin{equation}
        var(X + Y) = var(X) + 2 cov(X, Y) + var(Y),
    \end{equation}
    or, equivalently,
    \begin{equation}
        var(X + Y) = \sigma^2(X) + 2 \sigma(X) \sigma(Y) corr(X, Y) +
            \sigma^2(Y),
    \end{equation}
    where $ \sigma(X) $ and $ \sigma(Y) $ are the standard deviation of $ X $
        and $ Y $, respectively.
\end{lemma}

The correlation of two random variables contains the same information as the
    covariance, in terms of its sign, but its size is also relevant, since it
    is scaled to adjust for multiplication by constants, i.e.,
\begin{equation}
    corr(c_1 X, c_2 Y) = sign(c_1 c_2) corr(X, Y), \quad
        \forall c_1, c_2 \in \mathbb{R}.
\end{equation}

\begin{lemma}
    Let $ X $ and $ Y $ be two random variables over the same probability space.
    Then
    \begin{equation}
        -1 \leq corr(X, Y) \leq 1.
    \end{equation}
\end{lemma}

\begin{lemma}
    Let $ X_i $, $ i = 1 : n $, be random variables over the same probability
        space, and let $ c_i \in \mathbb{R} $ be real number.
    Then,
    \begin{align}
        E \left[ \sum_{i=1}^{n} c_i X_i \right] &=
            \sum_{i=1}^{n} c_i E[X_i]; \\
        var \left( \sum_{i=1}^{n} c_i X_i \right) &=
            \sum_{i=1}^{n} c_i^2 var(X_i) + 2 \sum_{i \leq i < j \leq n}
            c_i c_j cov(X_i, X_j).
        \label{eq:multi-var-variance}
    \end{align}
    If $ \rho_{i, j} = corr(X_i, X_j) $, $ 1 \leq i < j \leq n $, is the
        correlation between $ X_i $ and $ X_j $, and if
        $ \sigma_i^2 = var(X_i) $, $ i = 1 : n $, then
        \eqref{eq:multi-var-variance} can be written as
    \begin{equation}
        var \left( \sum_{i=1}^{n} c_i X_i \right) =
            \sum_{i=1}^{n} c_i^2 \sigma_i^2 +
            2 \sum_{i \leq i < j < \leq n} c_i c_j \sigma_i \sigma_j
            corr(X_i, X_j).
    \end{equation}
\end{lemma}

\section{The standard normal variable}
The standard normal variable, denoted by $ Z $, is the random variable with
    probability density function
\begin{equation}
    f(x) = \frac{1}{\sqrt{2 \pi}} e^{-\frac{x^2}{2}}.
    \label{eq:standard-pdf}
\end{equation}

\begin{lemma}
    The standard normal variable has mean 0 and variance 1, i.e.,
    \begin{align}
        E[Z] &= 0; \label{eq:standard-expected-value} \\
        var(Z) &= 1. \label{eq:standard-variance}
    \end{align}
\end{lemma}

\begin{lemma}
    If $ Z $ is the standard normal variable, then $ E[Z^2] = 1 $.
\end{lemma}

\begin{definition}
    Denote by $ N(t) $ the cumulative distribution of the standard normal
        variable $ Z $.
    Then,
    \begin{equation}
        N(t) = P(Z \leq t) = \frac{1}{\sqrt{2 \pi}} \int_{-\infty}^{t}
            e^{-\frac{x^2}{2}} dx.
    \end{equation}
\end{definition}

\begin{lemma}
    Let $ Z $ be the standard normal variable.
    Then,
    \begin{equation}
        P(Z \geq a) = P(Z \leq -a), \quad \forall a \in \mathbb{R}.
    \end{equation}
    In other words, if $ N(t) $ is the cumulative distribution of $ Z $, then
    \begin{equation}
        1 - N(a) = N(-a), \quad \forall a \in \mathbb{R}.
    \end{equation}
\end{lemma}

\section{Normal random variables}
\begin{definition}
    The random variable $ X $ is a normal variable if and only if
    \begin{equation}
        X = \mu + \sigma Z,
    \end{equation}
    where $ Z $ is the standard normal variable and
        $ \mu, \sigma \in \mathbb{R} $
\end{definition}

\begin{lemma}
    Let $ X = \mu + \sigma Z $ be a normal variable.
    Then
    \begin{align}
        E[X] &= \mu; \\
        var(X) &= \sigma^2; \\
        \sigma(X) = | \sigma |.
    \end{align}
\end{lemma}

\begin{lemma}
    Let $ X = \mu + \sigma Z $ be a normal variable, with $ \sigma \neq 0 $, and
        denote by $ h(x) $ the probability density function of $ X $.
    Then,
    \begin{equation}
        h(x) = \frac{1}{| \sigma  \sqrt{2 \pi}} \exp \left(
            -\frac{(x - \mu)^2}{2 \sigma^2} \right).
    \end{equation}
\end{lemma}

\section{The Black-Scholes formula}
The Black-Scholes formulas give the price of plain vanilla European call and put
    options, under the assumption that the price of the underlying asset has
    lognormal distribution.
For any values of $ t_1 $ and $ t_2 $ with $ t_1 < t_2 $, the random variable
    $ \frac{S(t_2)}{S(t_1)} $ is lognormal with paramaters
    $ \left( \mu - q - \frac{\sigma^2}{2} \right) (t_2 - t_1) $ and
    $ \sigma^2 (t_2 - t_1) $, i.e.,
\begin{equation}
    \ln \left( \frac{S(t_2)}{S(t_1)} \right) = \left( \mu - q -
        \frac{\sigma^2}{2} \right) (t_2 - t_1) + \sigma \sqrt{t_2 - t_1} Z,
\end{equation}
where $ Z $ is the standard normal variable.
The constants $ \mu $ and $ \sigma $ are called the drift and the volatility of
    the price $ S(t) $ of the underlying asset and represent the expected value
    and the standard deviation of the returns of the asset; $ q $ is the
    continuous rate at which the asset pays dividends.
